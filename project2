# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W8f8ut54iu3o3RBWhehLkKvzBvMZyAEt


"""

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.optimizers import Adam
from keras.losses import categorical_crossentropy
import matplotlib.pyplot as plt

# Step 1
input_shape = (100, 100, 3)


train_dir = '/content/drive/MyDrive/Colab Notebooks/data/test'
validation_dir = '/content/drive/MyDrive/Colab Notebooks/data/validation'
test_dir = '/content/drive/MyDrive/Colab Notebooks/data/test'

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

validation_datagen = ImageDataGenerator(rescale=1./255)


batch_size = 32

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=input_shape[:2],
    batch_size=batch_size,
    class_mode='categorical'
)

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=input_shape[:2],
    batch_size=batch_size,
    class_mode='categorical'
)


# Step 2
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


# Step 3
epochs = 10

def create_model(activation_conv, activation_dense, filters, optimizer_type):
    model = Sequential()
    model.add(Conv2D(filters, (3, 3), activation=activation_conv, input_shape=input_shape))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(filters * 2, (3, 3), activation=activation_conv))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation=activation_dense))
    model.add(Dropout(0.5))
    model.add(Dense(4, activation='softmax'))
    model.compile(optimizer=optimizer_type, loss='categorical_crossentropy', metrics=['accuracy'])

    return model


activation_conv_options = ['relu', 'LeakyReLU']
activation_dense_options = ['relu', 'elu']
filters_options = [32, 64, 128]
optimizer_options = ['adam', 'sgd'] 


val_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=input_shape[:2],
    batch_size=32,
    class_mode='categorical'
)


for activation_conv in activation_conv_options:
    for activation_dense in activation_dense_options:
        for filters in filters_options:
            for optimizer_type in optimizer_options:
               
                model = create_model(activation_conv, activation_dense, filters, optimizer_type)

                
                history = model.fit(
                    train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=epochs,
                    validation_data=val_generator,
                    validation_steps=len(val_generator),
                    verbose=0  
                )

                loss, accuracy = model.evaluate(val_generator, steps=len(val_generator), verbose=0)
                print(f"Activation Conv: {activation_conv}, Activation Dense: {activation_dense}, Filters: {filters}, Optimizer: {optimizer_type}")
                print(f"Validation Accuracy: {accuracy}, Validation Loss: {loss}")

# Step 4

history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator
)


plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

# Step 5

from keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt


test_image_paths = [
    '/content/drive/MyDrive/Colab Notebooks/data/test/Medium/Crack__20180419_06_19_09,915.bmp',
    '/content/drive/MyDrive/Colab Notebooks/data/test/Large/Crack__20180419_13_29_14,846.bmp'
    ]

for test_image_path in test_image_paths:
    test_image = image.load_img(test_image_path, target_size=input_shape[:2])
    test_image_array = image.img_to_array(test_image)
    test_image_array = np.expand_dims(test_image_array, axis=0)
    test_image_array /= 255.


    predictions = model.predict(test_image_array)
    predicted_class = np.argmax(predictions)
    class_labels = {0: 'small', 1: 'medium', 2: 'large', 3: 'none'}
    predicted_label = class_labels[predicted_class]
    plt.imshow(test_image)
    plt.axis('off')

    text = f'Large Crack: {predictions[0, 2]:.2%}\nMedium Crack: {predictions[0, 1]:.2%}\nSmall Crack: {predictions[0, 0]:.2%}\nNo Crack: {predictions[0, 3]:.2%}'
    plt.annotate(text, xy=(0, 0), xytext=(0, 0), ha='left', va='top', fontsize=10, color='green')
    plt.show()
    
    
    
    
    
    
